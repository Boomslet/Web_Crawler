# Web_Crawler
A scalable, open-source webcrawler that writes website data to file while crawling each new webpage

# Installation
##### Clone the Web_Crawler repository:
```
$ git clone https://github.com/Boomslet/Web_Crawler
```

# Usage
##### 1. Install required packages
```
$ pip install -r requirements.txt
```
##### 2. Run controller.py
```
%Run controller.py
```

##### 3. Call crawl(*urls) with desired URL(s):
```Python
>>> crawl('https://github.com/')
```

##### 4. Crawl!
```Python
Successfully crawled https://github.com/
Successfully crawled https://github.com/#start-of-content
Successfully crawled https://github.com/features
Successfully crawled https://github.com/business
Successfully crawled https://github.com/pricing
Successfully crawled https://github.com/dashboard
``` 
